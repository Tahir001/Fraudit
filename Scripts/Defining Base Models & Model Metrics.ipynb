{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da856731",
   "metadata": {},
   "source": [
    "# **IMI BIG DATA & AI CASE COMPETITION**\n",
    "\n",
    "## *By: Hafsa, Cindy, Tahir & Albert*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925b62c9",
   "metadata": {},
   "source": [
    "Before we start training our models, it is best to understand what metrics we will be using and makes sense for our business use case. One of the most common metrics used in Machine Learning Problems is Accuracy, or the number of times the predicted value was equivelent to the ground truth. <br>\n",
    "\n",
    "However, from EDA we know that we have a Class Imbalance issue so accuracy is flawed. In this notebook, we look into some other metrics that we can use, and implement our own metric based on domain knowledge. We also develop some baseline models as a bench mark for what's to come.\n",
    "\n",
    "<br> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "355bc0fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/var/folders/hc/rv0ch5hn293cmmlwhw7w_drh0000gn/T/ipykernel_4745/2940248913.py:48: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    }
   ],
   "source": [
    "# Import relevent Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gc\n",
    "import math\n",
    "\n",
    "# Model Metrics & Data Pre-processing \n",
    "from scipy import stats\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, classification_report, precision_recall_curve\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "#Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "#import lightgbm and xgboost \n",
    "import lightgbm as lgb \n",
    "import xgboost as xgb \n",
    "\n",
    "# Imbalance dataset methods\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "# Miscellaneous\n",
    "from collections import Counter\n",
    "\n",
    "# Additional Libraries -- Automatic Explanatory Data Analysis\n",
    "from pandas_profiling import ProfileReport\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "# Remove warnings (so it doesn't take up space)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set seed for repition \n",
    "np.random.seed(2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cace5d1b",
   "metadata": {},
   "source": [
    " <font size=\"4\"> Recall that we mentioned accuracy is inaccurate in the case of class imbalance problems. </font> \n",
    "\n",
    " <font size=\"4\"> Hence, we will be taking a more holistic approach, and looking at the following evaluation metrics:</font> <br>\n",
    "\n",
    "- Accuracy\n",
    "- Confusion Matrix\n",
    "- Percision (P)\n",
    "- Recall (R)\n",
    "- F1 Score (F1)\n",
    "- Area under the ROC, or simply AUC\n",
    "- Log loss\n",
    "- Sensitivity, True Positive Rate (How well the positive class was predicted) \n",
    "- Specificity, True Negative Rate (How well the negative class was predicted)\n",
    "- G-Mean = sqrt(sensitivity * specificity), it combines both sensitivity and specificity\n",
    "- Custom Loss Function\n",
    "\n",
    "Most of the metrics mentioned here are built in python already. <br>However, we define the custom loss function below. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6691ff3e",
   "metadata": {},
   "source": [
    "## Metrics - Custom Loss Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ad9f7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53af61c3",
   "metadata": {},
   "source": [
    "## Define Base Models\n",
    "\n",
    "We chose two simple baseline models (one linear, and one non-linear) to help with the next few phases of the pipe line.\n",
    "\n",
    "The two baseline models are:\n",
    "\n",
    "- Logistic Regression\n",
    "- Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce1a41e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Name</th>\n",
       "      <th>Train Accuracy</th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>TP</th>\n",
       "      <th>TN</th>\n",
       "      <th>FN</th>\n",
       "      <th>FP</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>AUC</th>\n",
       "      <th>G-Mean</th>\n",
       "      <th>Log-Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Model Name, Train Accuracy, Test Accuracy, TP, TN, FN, FP, F1-Score, AUC, G-Mean, Log-Loss]\n",
       "Index: []"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Initialize empty lists for results\n",
    "from sklearn.metrics import log_loss\n",
    "model_name, train_acc, test_acc, logLoss = [], [], [], []\n",
    "TN_lst, FN_lst, TP_lst, FP_lst, F1_SCORE, AUC, G_Mean = [], [], [], [], [], [], []\n",
    "\n",
    "def run_base_models(data, cols_to_drop):\n",
    "    \n",
    "    # Set seed for reproducability by setting random_state to 2022. \n",
    "    \n",
    "    # Get rid of the row column, and the Final IG label. \n",
    "    all_features = data.drop(axis=1, labels =cols_to_drop)\n",
    "    all_targets = all_features.pop(\"B_PLUS_FLAG\")\n",
    "    train_features, test_features, train_targets, test_targets = train_test_split(all_features, all_targets, test_size=0.2, random_state=2022)\n",
    "    \n",
    "    # Initialize all models in a list\n",
    "    models = [LogisticRegression(),\n",
    "              DecisionTreeClassifier(max_depth=8),\n",
    "             ]\n",
    "    # Define all the model names\n",
    "    model_names = [\"Logistic Regression\",\n",
    "                   \"Decision Tree\",\n",
    "                  ]\n",
    "    # Print the data size\n",
    "    print(\"Training Data size: {}\".format(train_features.shape))\n",
    "    print(\"Total Number of class labels in Test Set:\\n\", test_targets.value_counts())\n",
    "\n",
    "    # Loop over models instead of having separate cell per model\n",
    "    for name, model in zip(model_names, models):\n",
    "        # Training and model accuracy\n",
    "        model.random_state = 0\n",
    "        print(\"Training Model :  {}\".format(name))\n",
    "        model.fit(train_features, train_targets)\n",
    "        print(\"Done Training {}\".format(name))\n",
    "        test_score = model.score(test_features, test_targets) * 100\n",
    "        train_score = model.score(train_features, train_targets) * 100\n",
    "\n",
    "        # Predict Y values and see the TP, FP, et c(Using confusion matrix)\n",
    "        y_pred = model.predict(test_features) # removed the predict.probabilities\n",
    "        precision, recall, thresholds = metrics.precision_recall_curve(test_targets, y_pred)\n",
    "        pr_auc = metrics.auc(recall, precision)\n",
    "        f1score = f1_score(test_targets, y_pred)\n",
    "        CM = confusion_matrix(test_targets, y_pred)\n",
    "        TN, FN, TP, FP = CM[0][0], CM[1][0],  CM[1][1], CM[0][1]\n",
    "        Sensitivity, Specifity = (TP / (TP + FP)) , (TN / (FP + TN))\n",
    "        Gmean = np.sqrt(Sensitivity * Specifity)\n",
    "        logloss = log_loss(test_targets, y_pred)\n",
    "        \n",
    "        # Store results\n",
    "        model_name.append(name)\n",
    "        train_acc.append(train_score)\n",
    "        test_acc.append(test_score)\n",
    "        TN_lst.append(TN)\n",
    "        FN_lst.append(FN)\n",
    "        TP_lst.append(TP)\n",
    "        FP_lst.append(FP)\n",
    "        F1_SCORE.append(f1score)\n",
    "        AUC.append(pr_auc)\n",
    "        G_Mean.append(Gmean)\n",
    "        logLoss.append(logloss)    \n",
    "        \n",
    "    return None\n",
    "\n",
    "#drop_cols = [\"ROW\", \"Final_IG\", \"Date\"]\n",
    "#run_base_models(df1, drop_cols)\n",
    "\n",
    "results_dict = {\"Model Name\": model_name, \"Train Accuracy\": train_acc, \"Test Accuracy\": test_acc, \"TP\": TP_lst, \"TN\": TN_lst, \"FN\": FN_lst, \"FP\": FP_lst, \"F1-Score\": F1_SCORE, \"AUC\":AUC, \"G-Mean\": G_Mean, \"Log-Loss\": logLoss}\n",
    "results_df = pd.DataFrame.from_dict(results_dict)\n",
    "results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
